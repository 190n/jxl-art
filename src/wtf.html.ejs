<!DOCTYPE html>
<title>JXL Art</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="manifest" href="<%= emitEjs('src/manifest.json.ejs') %>" />
<link rel="stylesheet" href="<%= emitAsset('src/styles.css') %>" />
<style>
  html,
  body {
    background: #ddd;
  }
</style>

<body class="text">
  <p>
    <strong
      >JXL Art is the practice of using
      <a href="https://jpeg.org/jpegxl/">JPEG XL</a>’s prediction tree to
      generate art</strong
    >. If you have questions, you can join the <code>#jxl-art</code> channel on
    the <a href="https://discord.gg/DqkQgDRTFu">JPEG XL Discord</a>.
  </p>

  <p>
    The oversimplified summary is that JPEG XL has a modular mode that divides
    the image it encodes into squares, up to 1024x1024. JPEG XL uses a
    prediction tree to make a prediction what the value of each pixel is in such
    a square, based on neighboring pixels and their gradients. As a result only
    the <em>difference</em> (or error) between the <em>actual</em> image and the
    prediction needs to be encoded. The better the predictions, the smaller the
    error, the more compressible the data, the smaller the file size. Profit.
  </p>

  <p>
    In the context of JXL art, however,
    the error is always assumed to be zero, which makes the image consist of
    <em>only</em> the prediction tree. That means the predictions effectively
    generate the image. The process of creating JXL art is writing that
    prediction tree. A prediction tree is a tree of if-else statements,
    branching on different properties and selecting a predictor for each leaf of
    the tree. The flexibility of these prediction trees is intentionally limited
    because they need to be small and execution needs to be fast, as it is part
    of the image decoding process. The prediction tree is run for every channel
    (red, green and blue) and for every pixel. Some predictors incorporate the
    values of the current pixel’s neighbors, specifically to the left, top-left,
    top and top-right, which dictates in which order the pixels have to be
    predicted: Row-by-row, top-to-bottom, left-to-right — just like Westerners
    read text.
  </p>

  <p>
    A program starts with an optional header that specifies image properties and
    optional transformations to apply. The default header looks like this (everything is optional):
  </p>
  <pre>
Width 1024
Height 1024
RCT 0          /* Reversible Color Transform. 0 is no transform, so just RGB. 1 means YCoCg. Higher numbers can be used. */
/* XYB */      /* if this flag is added, the XYB color space is used instead of RGB. */
/* CbYCr */    /* if this flag is added, the YCbCr transform is applied. Channel 0 becomes Cb, channel 1 is Y, channel 2 is Cr. */
GroupShift 3   /* The group size is equal to 128 << GroupShift, so default is 1024. Values 0-3 are valid. */
Bitdepth 8     /* Other bit depths can be used, from 1 to 31. */
/* FloatExpBits 3 */  /* if this option is used, the numbers are interpreted as IEEE floats with this many exponent bits. */
/* Alpha */           /* if this flag is added, a fourth channel is added for Alpha. */
/* Squeeze */         /* if this flag is added, the Squeeze transform will be applied. Weird things will happen and the image gets many channels. */  
  </pre>
  <p>
    It is then followed by a tree description,
    which starts with a decision node — an if-else-like statement.
    Technically you can also just give a single predictor, but that’s not very
    useful to generate art, now is it (though it can be).
    A decision node looks like this:
  </p>

  <pre>
if [property] > [value:int]
   (THEN BRANCH)
   (ELSE BRANCH)
</pre
  >

  <p>
    Both the <code>THEN</code> branch and the <code>ELSE</code> branch can
    either be another decision node or a leaf node.
  </p>

  <p>The following properties can be used in a decision node:</p>

  <ul>
    <li><code>c</code>: the channel number, where 0=R, 1=G, 2=B, 3=A (if Alpha was enabled in the header)</li>
    <li><code>g</code>: the group number (useful in case the image is larger than one group). Modular group numbers usually start with 21.</li>
    <li><code>x</code>,<code>y</code>: coordinates</li>
    <li><code>N</code>: value of pixel above (north)</li>
    <li><code>W</code>: value of pixel to the left (west)</li>
    <li><code>|N|</code>: absolute value of pixel above (north)</li>
    <li><code>|W|</code>: absolute value of pixel to the left (west)</li>
    <li>
      <code>W-WW-NW+NWW</code>: basically the error of the gradient predictor
      for the pixel on the left
    </li>
    <li><code>W+N-NW</code>: value of gradient predictor (before clamping)</li>
    <li>
      <code>W-NW</code>: left minus topleft, i.e. error of the
      <code>N</code> predictor for the pixel on the left
    </li>
    <li>
      <code>NW-N</code>: topleft minus top, i.e. error of
      <code>W</code> predictor for the pixel above
    </li>
    <li>
      <code>N-NE</code>: top minus topright, i.e. error of
      <code>W</code> predictor for pixel on top right
    </li>
    <li>
      <code>N-NN</code>: top minus toptop, i.e. error of
      <code>N</code> predictor for pixel above
    </li>
    <li>
      <code>W-WW</code>: left minus leftleft, i.e. error of
      <code>W</code> predictor for the pixel on the left
    </li>
    <li><code>WGH</code>: signed max-absval-error of the weighted predictor</li>
    <li><code>Prev</code>: the pixel value in this position in the <i>previous</i> channel</li>
    <li><code>PPrev</code>: the pixel value in this position in the channel before the previous channel</li>
    <li><code>PrevErr</code>: the difference between pixel value and the <code>Gradient</code>-predicted value in this position in the <i>previous</i> channel</li>
    <li><code>PPrevErr</code>: like <code>PrevErr</code>, but for the channel before that</li>
    <li><code>PrevAbs, PPrevAbs, PrevAbsErr, PPrevAbsErr</code>: same as the abov, but the absolute value (not the signed value)</li>
  </ul>

  <p>Leaf nodes are of the following form:</p>

  <pre>
  - [predictor] +-[offset:int]
</pre
  >

  <p>The following predictors are supported in leaf nodes:</p>

  <ul>
    <li>
      <code>Set</code>: always predicts zero, so effectively sets the pixel
      value to [offset]
    </li>
    <li><code>W</code>: value of pixel on the left</li>
    <li><code>N</code>: value of pixel above</li>
    <li><code>NW</code>: value of topleft pixel</li>
    <li><code>NE</code>: value of topright pixel</li>
    <li>
      <code>WW</code>: value of pixel to the left of the pixel on the left
    </li>
    <li><code>Select</code>: predictor from lossless WebP</li>
    <li>
      <code>Gradient</code>: <code>W+N-NW</code>, clamped to
      <code>min(W,N)..max(W,N)</code>
    </li>
    <li>
      <code>Weighted</code>: weighted sum of 4 self-correcting subpredictors
      based on their past performance (warning: not clamped so can get out of
      range)
    </li>
    <li>
      <code>AvgW+N</code
      >,<code>AvgW+NW</code>,<code>AvgN+NW</code>,<code>AvgN+NE</code>: average
      of two pixel values
    </li>
    <li>
      <code>AvgAll</code>: weighted sum of various pixels:
      <code
        >(6 * top - 2 * toptop + 7 * left + 1 * leftleft + 1 * toprightright + 3
        * topright + 8) / 16</code
      >
    </li>
  </ul>

  <p>Edge cases:</p>

  <ul>
    <li>
      If <code>x=y=0</code>, <code>W</code> is set to 0. Otherwise, if
      <code>x=0</code>, <code>W</code> is set to <code>N</code>.
    </li>
    <li>If <code>y=0</code>, <code>N</code> is set to <code>W</code>.</li>
    <li>
      If <code>x=0</code> or <code>y=0</code>, <code>NW</code> is set to
      <code>W</code>.
    </li>
    <li>
      Similarly, <code>NE</code> and <code>NN</code> fall back to
      <code>N</code> and <code>WW</code> falls back to <code>W</code>.
    </li>
  </ul>
  <footer>
    Commit <%= meta.currentCommit %>
    <button class="button" id="nuke">Nuke cache ☣️</button>
  </footer>
  <script src="<%= emitChunk('src/wtf.js'); %>"></script>
</body>
