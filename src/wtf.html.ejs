<!DOCTYPE html>
<title>JXL Art</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="manifest" href="<%= emitEjs('src/manifest.json.ejs') %>" />
<link rel="stylesheet" href="<%= emitAsset('src/styles.css') %>" />
<style>
  html,
  body {
    background: #ddd;
  }
</style>

<body class="text">
  <p>
    <strong
      >JXL Art is the practice of using
      <a href="https://jpeg.org/jpegxl/">JPEG XL</a>’s prediction tree to
      generate art</strong
    >. If you have questions, you can join the <code>#jxl-art</code> channel on
    the <a href="https://discord.gg/DqkQgDRTFu">JPEG XL Discord</a>.
  </p>

  <p>
    The oversimplified summary is that JPEG XL has a modular mode that divides
    the image it encodes into squares, up to 1024x1024. JPEG XL uses a
    prediction tree to make a prediction what the value of each pixel is in such
    a square, based on neighboring pixels and their gradients. As a result only
    the <em>difference</em> (or error) between the <em>actual</em> image and the
    prediction needs to be encoded. The better the predictions, the smaller the
    error, the more compressible the data, the smaller the file size. Profit.
  </p>

  <p>
    In the context of JXL art, however, there is only one 1024x1024 square and
    the error is always assumed to be zero, which makes the image consist of
    <em>only</em> the prediction tree. That means the predictions effectively
    generate the image. The process of creating JXL art is writing that
    prediction tree. A prediction tree is a tree of if-else statements,
    branching on different properties and selecting a predictor for each leaf of
    the tree. The flexibility of these prediction trees is intentionally limited
    because they need to be small and execution needs to be fast, as it is part
    of the image decoding process. The prediction tree is run for every channel
    (red, green and blue) and for every pixel. Some predictors incorporate the
    values of the current pixel’s neighbors, specifically to the left, top-left,
    top and top-right, which dictates in which order the pixels have to be
    predicted: Row-by-row, top-to-bottom, left-to-right — just like Westerners
    read text.
  </p>

  <p>
    A program starts with a decision node — an if-else-like statement.
    Technically you can also just give a single predictor, but that’s not very
    useful to generate art, now is it. A decision node looks like this:
  </p>

  <pre>
if [property] > [value:int]
   (THEN BRANCH)
   (ELSE BRANCH)
</pre
  >

  <p>
    Both the <code>THEN</code> branch and the <code>ELSE</code> branch can
    either be another decision node or a leaf node.
  </p>

  <p>The following properties can be used in a decision node:</p>

  <ul>
    <li><code>c</code>: the channel number (0=R, 1=G, 2=B)</li>
    <li><code>x</code>,<code>y</code>: coordinates</li>
    <li><code>N</code>: value of pixel above (north)</li>
    <li><code>W</code>: value of pixel to the left (west)</li>
    <li>
      <code>W-WW-NW+NWW</code>: basically the error of the gradient predictor
      for the pixel on the left
    </li>
    <li><code>W+N-NW</code>: value of gradient predictor (before clamping)</li>
    <li>
      <code>W-NW</code>: left minus topleft, i.e. error of the
      <code>N</code> predictor for the pixel on the left
    </li>
    <li>
      <code>NW-N</code>: topleft minus top, i.e. error of
      <code>W</code> predictor for the pixel above
    </li>
    <li>
      <code>N-NE</code>: top minus topright, i.e. error of
      <code>W</code> predictor for pixel on top right
    </li>
    <li>
      <code>N-NN</code>: top minus toptop, i.e. error of
      <code>N</code> predictor for pixel above
    </li>
    <li>
      <code>W-WW</code>: left minus leftleft, i.e. error of
      <code>W</code> predictor for the pixel on the left
    </li>
    <li><code>WGH</code>: signed max-absval-error of the weighted predictor</li>
  </ul>

  <p>Leaf nodes are of the following form:</p>

  <pre>
  - [predictor] +-[offset:int]
</pre
  >

  <p>The following predictors are supported in leaf nodes:</p>

  <ul>
    <li>
      <code>Set</code>: always predicts zero, so effectively sets the pixel
      value to [offset]
    </li>
    <li><code>W</code>: value of pixel on the left</li>
    <li><code>N</code>: value of pixel above</li>
    <li><code>NW</code>: value of topleft pixel</li>
    <li><code>NE</code>: value of topright pixel</li>
    <li>
      <code>WW</code>: value of pixel to the left of the pixel on the left
    </li>
    <li><code>Select</code>: predictor from lossless WebP</li>
    <li>
      <code>Gradient</code>: <code>W+N-NW</code>, clamped to
      <code>min(W,N)..max(W,N)</code>
    </li>
    <li>
      <code>Weighted</code>: weighted sum of 4 self-correcting subpredictors
      based on their past performance (warning: not clamped so can get out of
      range)
    </li>
    <li>
      <code>AvgW+N</code
      >,<code>AvgW+NW</code>,<code>AvgN+NW</code>,<code>AvgN+NE</code>: average
      of two pixel values
    </li>
    <li>
      <code>AvgAll</code>: weighted sum of various pixels:
      <code
        >(6 * top - 2 * toptop + 7 * left + 1 * leftleft + 1 * toprightright + 3
        * topright + 8) / 16</code
      >
    </li>
  </ul>

  <p>Edge cases:</p>

  <ul>
    <li>
      If <code>x=y=0</code>, <code>W</code> is set to 0. Otherwise, if
      <code>x=0</code>, <code>W</code> is set to <code>N</code>.
    </li>
    <li>If <code>y=0</code>, <code>N</code> is set to <code>W</code>.</li>
    <li>
      If <code>x=0</code> or <code>y=0</code>, <code>NW</code> is set to
      <code>W</code>.
    </li>
    <li>
      Similarly, <code>NE</code> and <code>NN</code> fall back to
      <code>N</code> and <code>WW</code> falls back to <code>W</code>.
    </li>
  </ul>
</body>
